{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dataclasses import dataclass\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xlrd == 1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LogisticRegression:\n",
    "    learningRate: float\n",
    "    tolerance: float\n",
    "    maxIteration: int\n",
    "    filePath: str\n",
    "    \n",
    "    def __post_int__(self):\n",
    "        self.train_X, self.rain_y, self.test_X, self.test_y = self.readDataset()\n",
    "        self.train_X = self.addX0(self.train_X)\n",
    "        self.test_X = self.addX0(self.test_X)\n",
    "        \n",
    "    def readDataset(self):\n",
    "        train_df = pd.read_excel(self.filePath, sheet_name='2004--2005 Data')\n",
    "        test_df = pd.read_excel(self.filePath, sheet_name='2004--2007 Data')\n",
    "        train_df = train_df.values\n",
    "        test_df = train_df.values\n",
    "        \n",
    "        train_X, train_y = train_df[:,1:], train_df[:,0]\n",
    "        test_X, test_y = test_df[:,1:], test_df[:,0]\n",
    "        \n",
    "        return train_X, train_y, test_X, test_y\n",
    "    \n",
    "    def addX0 (self, X):\n",
    "        return np.column_stack([np.ones(X.shape[0]), X])\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        sig = 1 / (1 + np.exp(-z))\n",
    "        return sig\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = self.sigmoid(X.dot(self.w))\n",
    "        return pred\n",
    "    \n",
    "    def costFunction(self, X, y):\n",
    "        # approach 1\n",
    "        sig = self.predict(X)\n",
    "        pred = y * log(sig) + (1-y)* np.log(1-sig)\n",
    "        cost = - pred.sum()\n",
    "        \n",
    "        # approach 2\n",
    "        pred = np.log(np.ones(X.shape[0])-np.exp(X.dot(self.w))) - X.dot(self.w).dot(y)\n",
    "        cost = pred.sum()\n",
    "        return cost\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        sig = self.sigmoid(self.predict(X))\n",
    "        grad = (sig - y).dot(X)\n",
    "        return grad\n",
    "    \n",
    "    def gradientDescent(self, X, y):\n",
    "        error = []\n",
    "        last = float('inf')\n",
    "        \n",
    "        for i in tqdm(range (self.maxIteration)):\n",
    "            self.w = self.w - self.learningRate * self.gradient(X, y)\n",
    "            current = self.costFunction(X, y)\n",
    "            diff = abs(last - current)\n",
    "            last = current\n",
    "            if diff < self.tolerance:\n",
    "                print(\"model stopped learning\")\n",
    "                break\n",
    "            \n",
    "    def fit(self):\n",
    "        print('solve using Gradient Descent')\n",
    "        self.w = np.ones(self.train_X.shape[1], dtype = np.float64)\n",
    "        self.gradientDescent(self.train_X, self.train_y)\n",
    "        \n",
    "        f_score, precision, recall = self.evaluate(self.train_X, self.train_y)\n",
    "        \n",
    "        print(\"F1 score {} and precision {}, recall {}\".format(f_score, precision, recall))\n",
    "        \n",
    "    def evaluate(self, X, y):\n",
    "        # Precision and recall: refer to wikipedia\n",
    "        # F-measure when data is imbalanced\n",
    "        y_hat = self.predict(X)\n",
    "        y = (y == 1)\n",
    "        y_hat = (y_hat == 1)\n",
    "        \n",
    "        precision = (y & y_hat).sum() / y_hat.sum()\n",
    "        recall = (y & y_hat).sum() / y.sum()\n",
    "        \n",
    "        f_score = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        return f_score, precision, recall\n",
    "    \n",
    "    # def plot functions from lab 2 hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(0.00001, 0.0005, 50000, 'type_in_ur_file_name.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "004a5ef9e6b5c0f862341ac783ae70e73a637490667c34419a076afb42ad424d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
